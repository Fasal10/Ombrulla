{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPm7MArMaOj3fbNXQNhHQhn",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Fasal10/Ombrulla/blob/main/main.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "kdD1Mxe4wjY0"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import sys\n",
        "import json\n",
        "import logging\n",
        "from pathlib import Path\n",
        "from typing import List, Dict, Tuple, Optional\n",
        "from dataclasses import dataclass\n",
        "import argparse"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install cohere\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3ojTCuwA1eZ1",
        "outputId": "a17af665-80bc-4b47-f9af-1bb7e9a31e58"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting cohere\n",
            "  Downloading cohere-5.15.0-py3-none-any.whl.metadata (3.4 kB)\n",
            "Collecting fastavro<2.0.0,>=1.9.4 (from cohere)\n",
            "  Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: httpx>=0.21.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.28.1)\n",
            "Collecting httpx-sse==0.4.0 (from cohere)\n",
            "  Downloading httpx_sse-0.4.0-py3-none-any.whl.metadata (9.0 kB)\n",
            "Requirement already satisfied: pydantic>=1.9.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.11.4)\n",
            "Requirement already satisfied: pydantic-core<3.0.0,>=2.18.2 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.33.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<1,>=0.15 in /usr/local/lib/python3.11/dist-packages (from cohere) (0.21.1)\n",
            "Collecting types-requests<3.0.0,>=2.0.0 (from cohere)\n",
            "  Downloading types_requests-2.32.0.20250515-py3-none-any.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: typing_extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from cohere) (4.13.2)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (2025.4.26)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (1.0.9)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx>=0.21.2->cohere) (3.10)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.21.2->cohere) (0.16.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.7.0)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=1.9.2->cohere) (0.4.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (3.4.2)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.0.0->cohere) (2.4.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<1,>=0.15->cohere) (0.31.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (3.18.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (2025.3.2)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (6.0.2)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<1,>=0.15->cohere) (4.67.1)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx>=0.21.2->cohere) (1.3.1)\n",
            "Downloading cohere-5.15.0-py3-none-any.whl (259 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m259.5/259.5 kB\u001b[0m \u001b[31m9.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx_sse-0.4.0-py3-none-any.whl (7.8 kB)\n",
            "Downloading fastavro-1.11.1-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading types_requests-2.32.0.20250515-py3-none-any.whl (20 kB)\n",
            "Installing collected packages: types-requests, httpx-sse, fastavro, cohere\n",
            "Successfully installed cohere-5.15.0 fastavro-1.11.1 httpx-sse-0.4.0 types-requests-2.32.0.20250515\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install ultralytics"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lj_7-uhz1qrZ",
        "outputId": "b31373c4-77a0-4b95-a173-4ffc213e4a3b"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.145-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.0.2)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.6.0+cu124)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.21.0+cu124)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.4.26)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.13.2)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.4.127)\n",
            "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.8.0->ultralytics)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.145-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m22.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m59.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m33.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m38.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, ultralytics-thop, ultralytics\n",
            "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-curand-cu12\n",
            "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "  Attempting uninstall: nvidia-cufft-cu12\n",
            "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "  Attempting uninstall: nvidia-cublas-cu12\n",
            "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "  Attempting uninstall: nvidia-cusparse-cu12\n",
            "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "  Attempting uninstall: nvidia-cudnn-cu12\n",
            "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "  Attempting uninstall: nvidia-cusolver-cu12\n",
            "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 ultralytics-8.3.145 ultralytics-thop-2.0.14\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2\n",
        "import numpy as np\n",
        "from PIL import Image\n",
        "import cohere\n",
        "from ultralytics import YOLO"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nRF_HMyo2LPx",
        "outputId": "3a4eb9d9-a994-42bd-be8f-879f1e75db24"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating new Ultralytics Settings v0.0.6 file ✅ \n",
            "View Ultralytics Settings with 'yolo settings' or at '/root/.config/Ultralytics/settings.json'\n",
            "Update Settings with 'yolo settings key=value', i.e. 'yolo settings runs_dir=path/to/dir'. For help see https://docs.ultralytics.com/quickstart/#ultralytics-settings.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "@dataclass\n",
        "class DetectedObject:\n",
        "    \"\"\"Data class for detected objects\"\"\"\n",
        "    name: str\n",
        "    confidence: float\n",
        "    bbox: Tuple[int, int, int, int]  # (x1, y1, x2, y2)\n",
        "\n",
        "\n",
        "@dataclass\n",
        "class VisionResult:\n",
        "    \"\"\"Data class for vision analysis results\"\"\"\n",
        "    objects: List[DetectedObject]\n",
        "    image_description: str\n",
        "\n",
        "\n",
        "class VisionAnalyzer:\n",
        "    \"\"\"Computer vision module for object detection\"\"\"\n",
        "\n",
        "    def __init__(self, model_name: str = \"yolov8n.pt\"):\n",
        "        \"\"\"\n",
        "        Initialize the vision analyzer\n",
        "\n",
        "        Args:\n",
        "            model_name: YOLOv8 model name (yolov8n.pt, yolov8s.pt, etc.)\n",
        "        \"\"\"\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        try:\n",
        "            self.model = YOLO(model_name)\n",
        "            self.logger.info(f\"Loaded YOLO model: {model_name}\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to load YOLO model: {e}\")\n",
        "            raise\n",
        "    def detect_objects(self, image_path: str, confidence_threshold: float = 0.5) -> List[DetectedObject]:\n",
        "        try:\n",
        "            # Validate image file\n",
        "            if not os.path.exists(image_path):\n",
        "                raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "            # Run inference\n",
        "            results = self.model(image_path, conf=confidence_threshold)\n",
        "\n",
        "            detected_objects = []\n",
        "\n",
        "            for result in results:\n",
        "                boxes = result.boxes\n",
        "                if boxes is not None:\n",
        "                    for box in boxes:\n",
        "                        # Extract box data\n",
        "                        x1, y1, x2, y2 = box.xyxy[0].cpu().numpy()\n",
        "                        confidence = float(box.conf[0])\n",
        "                        class_id = int(box.cls[0])\n",
        "                        class_name = self.model.names[class_id]\n",
        "\n",
        "                        detected_objects.append(DetectedObject(\n",
        "                            name=class_name,\n",
        "                            confidence=confidence,\n",
        "                            bbox=(int(x1), int(y1), int(x2), int(y2))\n",
        "                        ))\n",
        "\n",
        "            self.logger.info(f\"Detected {len(detected_objects)} objects\")\n",
        "            return detected_objects\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Object detection failed: {e}\")\n",
        "            raise\n",
        "    def analyze_image(self, image_path: str, confidence_threshold: float = 0.5) -> VisionResult:\n",
        "\n",
        "        objects = self.detect_objects(image_path, confidence_threshold)\n",
        "\n",
        "        # Generate image description based on detected objects\n",
        "        if objects:\n",
        "            object_names = [obj.name for obj in objects]\n",
        "            unique_objects = list(set(object_names))\n",
        "            object_counts = {name: object_names.count(name) for name in unique_objects}\n",
        "\n",
        "            description_parts = []\n",
        "            for name, count in object_counts.items():\n",
        "                if count == 1:\n",
        "                    description_parts.append(f\"a {name}\")\n",
        "                else:\n",
        "                    description_parts.append(f\"{count} {name}s\")\n",
        "\n",
        "            if len(description_parts) == 1:\n",
        "                description = f\"The image contains {description_parts[0]}.\"\n",
        "            elif len(description_parts) == 2:\n",
        "                description = f\"The image contains {description_parts[0]} and {description_parts[1]}.\"\n",
        "            else:\n",
        "                description = f\"The image contains {', '.join(description_parts[:-1])}, and {description_parts[-1]}.\"\n",
        "        else:\n",
        "            description = \"No objects were detected in the image.\"\n",
        "\n",
        "        return VisionResult(objects=objects, image_description=description)"
      ],
      "metadata": {
        "id": "eV2zwszm2bHT"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "B9XR7Ko_5hbO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "bYRljP--4OJl"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class LLMProcessor:\n",
        "    \"\"\"LLM module for text generation using Cohere\"\"\"\n",
        "\n",
        "    def __init__(self, api_key: Optional[str] = None, model: str = \"command\"):\n",
        "\n",
        "        api_key = \"VF6cY1nbS3SWoz5L5SDG8vzlSMMt2hsqR8bE1SmN\"\n",
        "\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "        self.model = model\n",
        "\n",
        "        # Set up Cohere client\n",
        "        if api_key:\n",
        "            self.client = cohere.Client(api_key)\n",
        "        else:\n",
        "            cohere_api_key = os.getenv('COHERE_API_KEY')\n",
        "            if not cohere_api_key:\n",
        "                raise ValueError(\"Cohere API key not provided. Set COHERE_API_KEY environment variable or pass api_key parameter.\")\n",
        "            self.client = cohere.Client(cohere_api_key)\n",
        "\n",
        "        self.logger.info(f\"Initialized LLM processor with Cohere model: {model}\")\n",
        "    def generate_response(self, vision_result: VisionResult, user_prompt: str) -> str:\n",
        "        \"\"\"\n",
        "        Generate a text response combining vision analysis and user prompt\n",
        "\n",
        "        Args:\n",
        "            vision_result: Results from vision analysis\n",
        "            user_prompt: User-provided text prompt\n",
        "\n",
        "        Returns:\n",
        "            Generated text response\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Prepare context from vision analysis\n",
        "            objects_info = []\n",
        "            for obj in vision_result.objects:\n",
        "                objects_info.append(f\"- {obj.name} (confidence: {obj.confidence:.2f})\")\n",
        "\n",
        "            objects_list = \"\\n\".join(objects_info) if objects_info else \"No objects detected\"\n",
        "\n",
        "            # Construct the prompt for the LLM\n",
        "            # Combine the image description and user prompt\n",
        "            prompt = f\"Image Description: {vision_result.image_description}\\n\\n\"\n",
        "            prompt += f\"Detected Objects:\\n{objects_list}\\n\\n\"\n",
        "            prompt += f\"User Query: {user_prompt}\\n\\n\"\n",
        "            prompt += \"Based on the image analysis, respond to the user query.\" # Add an instruction\n",
        "\n",
        "            # Generate response using Cohere\n",
        "            response = self.client.generate(\n",
        "                model=self.model,\n",
        "                prompt=prompt, # Now 'prompt' is defined\n",
        "                max_tokens=500,\n",
        "                temperature=0.7,\n",
        "                k=0,\n",
        "                stop_sequences=[],\n",
        "                return_likelihoods='NONE'\n",
        "            )\n",
        "\n",
        "            # Extract the generated text\n",
        "            generated_text = response.generations[0].text.strip()\n",
        "\n",
        "            # The prompt includes \"Based on the image analysis...\", so no need to prepend again\n",
        "            # final_response = f\"Based on the image analysis, I can see{generated_text}\"\n",
        "            final_response = generated_text # Use the generated text directly\n",
        "\n",
        "            self.logger.info(\"Successfully generated Cohere response\")\n",
        "            return final_response\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Cohere generation failed: {e}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "JmCPpZ9j4bqg"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "class VisionLLMApp:\n",
        "    \"\"\"Main application class integrating vision and LLM capabilities\"\"\"\n",
        "\n",
        "    def __init__(self, yolo_model: str = \"yolov8n.pt\", llm_model: str = \"command\"):\n",
        "        \"\"\"\n",
        "        Initialize the integrated application\n",
        "\n",
        "        Args:\n",
        "            yolo_model: YOLO model name for object detection\n",
        "            llm_model: Cohere model name for text generation (command, command-light, command-nightly)\n",
        "        \"\"\"\n",
        "        # Set up logging\n",
        "        logging.basicConfig(\n",
        "            level=logging.INFO,\n",
        "            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
        "        )\n",
        "        self.logger = logging.getLogger(__name__)\n",
        "\n",
        "        # Initialize components\n",
        "        try:\n",
        "            self.vision_analyzer = VisionAnalyzer(yolo_model)\n",
        "            self.llm_processor = LLMProcessor(model=llm_model)\n",
        "            self.logger.info(\"VisionLLM application initialized successfully\")\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Failed to initialize application: {e}\")\n",
        "            raise\n",
        "\n",
        "    def process_request(self, image_path: str, text_prompt: str,\n",
        "                       confidence_threshold: float = 0.5) -> Dict:\n",
        "        \"\"\"\n",
        "        Process a complete request with image and text prompt\n",
        "\n",
        "        Args:\n",
        "            image_path: Path to the image file\n",
        "            text_prompt: User text prompt\n",
        "            confidence_threshold: Minimum confidence for object detection\n",
        "\n",
        "        Returns:\n",
        "            Dictionary containing results\n",
        "        \"\"\"\n",
        "        try:\n",
        "            # Validate inputs\n",
        "            self._validate_inputs(image_path, text_prompt)\n",
        "\n",
        "            # Perform vision analysis\n",
        "            self.logger.info(f\"Analyzing image: {image_path}\")\n",
        "            vision_result = self.vision_analyzer.analyze_image(image_path, confidence_threshold)\n",
        "\n",
        "            # Generate LLM response\n",
        "            self.logger.info(\"Generating LLM response\")\n",
        "            llm_response = self.llm_processor.generate_response(vision_result, text_prompt)\n",
        "\n",
        "            # Prepare results\n",
        "            results = {\n",
        "                \"image_path\": image_path,\n",
        "                \"user_prompt\": text_prompt,\n",
        "                \"detected_objects\": [\n",
        "                    {\n",
        "                        \"name\": obj.name,\n",
        "                        \"confidence\": round(obj.confidence, 3),\n",
        "                        \"bbox\": obj.bbox\n",
        "                    }\n",
        "                    for obj in vision_result.objects\n",
        "                ],\n",
        "                \"image_description\": vision_result.image_description,\n",
        "                \"llm_response\": llm_response,\n",
        "                \"status\": \"success\"\n",
        "            }\n",
        "\n",
        "            self.logger.info(\"Request processed successfully\")\n",
        "            return results\n",
        "\n",
        "        except Exception as e:\n",
        "            self.logger.error(f\"Request processing failed: {e}\")\n",
        "            return {\n",
        "                \"image_path\": image_path,\n",
        "                \"user_prompt\": text_prompt,\n",
        "                \"error\": str(e),\n",
        "                \"status\": \"error\"\n",
        "            }\n",
        "\n",
        "    def _validate_inputs(self, image_path: str, text_prompt: str):\n",
        "        \"\"\"Validate input parameters\"\"\"\n",
        "        if not image_path or not isinstance(image_path, str):\n",
        "            raise ValueError(\"Invalid image path provided\")\n",
        "\n",
        "        if not os.path.exists(image_path):\n",
        "            raise FileNotFoundError(f\"Image file not found: {image_path}\")\n",
        "\n",
        "        # Check if file is a valid image\n",
        "        valid_extensions = {'.jpg', '.jpeg', '.png', '.bmp', '.tiff', '.webp'}\n",
        "        if not any(image_path.lower().endswith(ext) for ext in valid_extensions):\n",
        "            raise ValueError(\"Invalid image file format. Supported formats: jpg, jpeg, png, bmp, tiff, webp\")\n",
        "\n",
        "        if not text_prompt or not isinstance(text_prompt, str):\n",
        "            raise ValueError(\"Invalid text prompt provided\")\n",
        "\n",
        "        if len(text_prompt.strip()) == 0:\n",
        "            raise ValueError(\"Text prompt cannot be empty\")\n",
        "\n",
        "def main():\n",
        "    \"\"\"Main function for command-line usage\"\"\"\n",
        "    parser = argparse.ArgumentParser(description=\"Vision-LLM Integration Application\")\n",
        "    parser.add_argument(\"image_path\", help=\"/content/car and bike.jpg\")\n",
        "    parser.add_argument( \"user_prompt\",  help=\"Write a story about the things detected from the image\")\n",
        "    parser.add_argument(\"--confidence\",  type=float,  default=0.5,  help=\"Confidence threshold for object detection (default: 0.5)\" )\n",
        "    parser.add_argument(\"--yolo-model\",  default=\"yolov8n.pt\",  help=\"YOLO model name to use for detection (default: yolov8n.pt)\" )\n",
        "    parser.add_argument(\"--llm-model\",   default=\"command\",  help=\"LLM model name to generate the story (default: command)\" )\n",
        "    parser.add_argument(\"--output\",  help=\"Output file path for saving results in JSON format\" )\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    try:\n",
        "        # Initialize application\n",
        "        app = VisionLLMApp(yolo_model=args.yolo_model, llm_model=args.llm_model)\n",
        "\n",
        "        # Process request\n",
        "        results = app.process_request(args.image_path, args.prompt, args.confidence)\n",
        "\n",
        "        # Output results\n",
        "        if results[\"status\"] == \"success\":\n",
        "            print(\"\\n\" + \"=\"*50)\n",
        "            print(\"VISION-LLM ANALYSIS RESULTS\")\n",
        "            print(\"=\"*50)\n",
        "            print(f\"\\nImage: {results['image_path']}\")\n",
        "            print(f\"Prompt: {results['user_prompt']}\")\n",
        "            print(f\"\\nImage Description: {results['image_description']}\")\n",
        "            print(f\"\\nDetected Objects ({len(results['detected_objects'])}):\")\n",
        "            for obj in results['detected_objects']:\n",
        "                print(f\"  - {obj['name']}: {obj['confidence']:.3f} confidence\")\n",
        "            print(f\"\\nLLM Response:\\n{results['llm_response']}\")\n",
        "        else:\n",
        "            print(f\"Error: {results['error']}\")\n",
        "            return 1\n",
        "\n",
        "        # Save to file if requested\n",
        "        if args.output:\n",
        "            with open(args.output, 'w') as f:\n",
        "                json.dump(results, f, indent=2)\n",
        "            print(f\"\\nResults saved to: {args.output}\")\n",
        "\n",
        "        return 0\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Application error: {e}\")\n",
        "        return 1\n"
      ],
      "metadata": {
        "id": "jdFUFDDg5RJ_"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "try:\n",
        "    # Initialize the application without argparse\n",
        "    app = VisionLLMApp(yolo_model=\"yolov8n.pt\", llm_model=\"command\")\n",
        "\n",
        "    # Define your image path and prompt\n",
        "    image_file_path = \"/content/car and bike.jpg\" # Replace with your actual image path\n",
        "    user_text_prompt = \"Write a story about the things detected from the image\"\n",
        "\n",
        "    # Process the request\n",
        "    analysis_results = app.process_request(image_file_path, user_text_prompt, confidence_threshold=0.5)\n",
        "\n",
        "    # Print the results\n",
        "    if analysis_results[\"status\"] == \"success\":\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\"VISION-LLM ANALYSIS RESULTS\")\n",
        "        print(\"=\"*50)\n",
        "        print(f\"\\nImage: {analysis_results['image_path']}\")\n",
        "        print(f\"Prompt: {analysis_results['user_prompt']}\")\n",
        "        print(f\"\\nImage Description: {analysis_results['image_description']}\")\n",
        "        print(f\"\\nDetected Objects ({len(analysis_results['detected_objects'])}):\")\n",
        "        for obj in analysis_results['detected_objects']:\n",
        "            print(f\"  - {obj['name']}: {obj['confidence']:.3f} confidence\")\n",
        "        print(f\"\\nLLM Response:\\n{analysis_results['llm_response']}\")\n",
        "    else:\n",
        "        print(f\"Error: {analysis_results['error']}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Application error during execution: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "We_Q4Mk99P4C",
        "outputId": "14d5b7f7-0e77-4885-dfca-aff2802d6b51"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "image 1/1 /content/car and bike.jpg: 416x640 1 car, 1 motorcycle, 188.0ms\n",
            "Speed: 4.3ms preprocess, 188.0ms inference, 1.8ms postprocess per image at shape (1, 3, 416, 640)\n",
            "\n",
            "==================================================\n",
            "VISION-LLM ANALYSIS RESULTS\n",
            "==================================================\n",
            "\n",
            "Image: /content/car and bike.jpg\n",
            "Prompt: Write a story about the things detected from the image\n",
            "\n",
            "Image Description: The image contains a motorcycle and a car.\n",
            "\n",
            "Detected Objects (2):\n",
            "  - motorcycle: 0.854 confidence\n",
            "  - car: 0.780 confidence\n",
            "\n",
            "LLM Response:\n",
            "The 85% confidence level detected motorcycle is flaunting its sleek design, proudly presenting itself in front of the 78% confidence level detected car. The motorcycle is a powerful machine, designed for speed and freedom on the roads, embracing the adventure of the open air as it drives by. The car, however, represents a more practical form of transportation, offering safety and comfort for travels long and short. Although these two vehicles differ in their appearances, they both share the same roadways and provide alternatives for efficient travel.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oZV1LC8_5i2s"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}